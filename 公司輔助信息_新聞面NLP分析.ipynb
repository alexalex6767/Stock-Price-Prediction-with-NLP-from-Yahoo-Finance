{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexalex6767/Stock-Price-Prediction-with-NLP-from-Yahoo-Finance/blob/main/%E5%85%AC%E5%8F%B8%E8%BC%94%E5%8A%A9%E4%BF%A1%E6%81%AF_%E6%96%B0%E8%81%9E%E9%9D%A2NLP%E5%88%86%E6%9E%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGTvQKZbp_Yj"
      },
      "source": [
        "## Steps\n",
        "1. Obtained **\"Specific Corporations\"** (variable) word data from some **time periods**(variable)，and used Yahoo finance API to obtain the corresponding **stock price**，Characteristic:\n",
        "    - Time\n",
        "    - News Source\n",
        "    - News Title\n",
        "    - News Content\n",
        "    - [signal](https://finance.yahoo.com/quote/3105.TWO/chart?p=3105.TWO#eyJpbnRlcnZhbCI6IndlZWsiLCJwZXJpb2RpY2l0eSI6MSwidGltZVVuaXQiOm51bGwsImNhbmRsZVdpZHRoIjoyMi4yNTQ5MDE5NjA3ODQzMTMsImZsaXBwZWQiOmZhbHNlLCJ2b2x1bWVVbmRlcmxheSI6dHJ1ZSwiYWRqIjp0cnVlLCJjcm9zc2hhaXIiOnRydWUsImNoYXJ0VHlwZSI6ImxpbmUiLCJleHRlbmRlZCI6ZmFsc2UsIm1hcmtldFNlc3Npb25zIjp7fSwiYWdncmVnYXRpb25UeXBlIjoib2hsYyIsImNoYXJ0U2NhbGUiOiJsaW5lYXIiLCJzdHVkaWVzIjp7IuKAjHZvbCB1bmRy4oCMIjp7InR5cGUiOiJ2b2wgdW5kciIsImlucHV0cyI6eyJpZCI6IuKAjHZvbCB1bmRy4oCMIiwiZGlzcGxheSI6IuKAjHZvbCB1bmRy4oCMIn0sIm91dHB1dHMiOnsiVXAgVm9sdW1lIjoiIzAwYjA2MSIsIkRvd24gVm9sdW1lIjoiI2ZmMzMzYSJ9LCJwYW5lbCI6ImNoYXJ0IiwicGFyYW1ldGVycyI6eyJ3aWR0aEZhY3RvciI6MC40NSwiY2hhcnROYW1lIjoiY2hhcnQifX19LCJwYW5lbHMiOnsiY2hhcnQiOnsicGVyY2VudCI6MSwiZGlzcGxheSI6IjMxMDUuVFdPIiwiY2hhcnROYW1lIjoiY2hhcnQiLCJpbmRleCI6MCwieUF4aXMiOnsibmFtZSI6ImNoYXJ0IiwicG9zaXRpb24iOm51bGx9LCJ5YXhpc0xIUyI6W10sInlheGlzUkhTIjpbImNoYXJ0Iiwi4oCMdm9sIHVuZHLigIwiXX19LCJsaW5lV2lkdGgiOjIsInN0cmlwZWRCYWNrZ3JvdW5kIjp0cnVlLCJldmVudHMiOnRydWUsImNvbG9yIjoiIzAwODFmMiIsInN0cmlwZWRCYWNrZ3JvdWQiOnRydWUsImV2ZW50TWFwIjp7ImNvcnBvcmF0ZSI6eyJkaXZzIjp0cnVlLCJzcGxpdHMiOnRydWV9LCJzaWdEZXYiOnt9fSwicmFuZ2UiOnsiZHRMZWZ0IjoiMjAyMC0xMi0zMVQxNjowMDowMC40NzJaIiwiZHRSaWdodCI6IjIwMjItMDEtMDJUMTU6NTk6MDAuNDcyWiIsInBlcmlvZGljaXR5Ijp7ImludGVydmFsIjoid2VlayIsInBlcmlvZCI6MX0sInBhZGRpbmciOjB9LCJjdXN0b21SYW5nZSI6eyJzdGFydCI6MTYwOTYwMzIwMDAwMCwiZW5kIjoxNjM5ODQzMjAwMDAwfSwic3ltYm9scyI6W3sic3ltYm9sIjoiMzEwNS5UV08iLCJzeW1ib2xPYmplY3QiOnsic3ltYm9sIjoiMzEwNS5UV08iLCJxdW90ZVR5cGUiOiJFUVVJVFkiLCJleGNoYW5nZVRpbWVab25lIjoiQXNpYS9UYWlwZWkifSwicGVyaW9kaWNpdHkiOjEsImludGVydmFsIjoid2VlayIsInRpbWVVbml0IjpudWxsfV19)\n",
        "2. Based on the data above to establish **Textual Classification Model(詳情可看論文)** (這邊就是一個完整專案流程)\n",
        "    - data preprocessing\n",
        "    - Characteristic Engineering\n",
        "    - Model Selection and Training\n",
        "    - Adjust parameter\n",
        "    - Deploy and Monitor\n",
        "3. Conclusion"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "kcanuNVtTzvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install selenium"
      ],
      "metadata": {
        "id": "rT6BG5M4T-AK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-01T20:44:59.586344Z",
          "start_time": "2022-01-01T20:44:37.012595Z"
        },
        "id": "Mel3C804p_Yp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import re\n",
        "import jieba\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import transformers\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "# 爬蟲相關套件\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import selenium\n",
        "from selenium import webdriver\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# tf-idf\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "\n",
        "plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei']\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-01T20:45:56.826652Z",
          "start_time": "2022-01-01T20:45:35.828799Z"
        },
        "id": "NILG-QLqp_Yr"
      },
      "outputs": [],
      "source": [
        "# 1. 取得特定公司的新聞文字資料\n",
        "# 透過 webdriver 模擬網頁開啟動作，可以模擬真實操作網頁，故可以省下許多AJAX or 其他爬蟲技巧，可以直覺地透過模擬搭配 bs4 萃取需要的網站資訊。\n",
        "\n",
        "\n",
        "# 參數設定\n",
        "company_num = '3105'\n",
        "url = f'https://tw.stock.yahoo.com/quote/{company_num}'\n",
        "\n",
        "\n",
        "# driver init\n",
        "driver = webdriver.Chrome('chromedriver.exe')\n",
        "\n",
        "# get in the url\n",
        "driver.get(url)   # 進入該網頁\n",
        "time.sleep(5)     # 等待載入完畢, 若沒有等待可能會造成有些元素還沒有respond\n",
        "\n",
        "# scroll down\n",
        "js=\"var q=document.documentElement.scrollTop=10000\"\n",
        "driver.execute_script(js)\n",
        "time.sleep(3)\n",
        "\n",
        "# 透過BeautifulSoup解析網頁內容\n",
        "soup = BeautifulSoup(driver.page_source, \"html.parser\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-01T20:46:00.092930Z",
          "start_time": "2022-01-01T20:46:00.006937Z"
        },
        "id": "bf58LV8Vp_Ys"
      },
      "outputs": [],
      "source": [
        "# 這邊有抓到一些不適用的東西, 可以透過 \"穩懋\" 判斷文章內容，沒有就剔除\n",
        "\n",
        "for e in soup.find_all(class_='js-stream-content Pos(r)'):\n",
        "    print(e.find_all('a')[0].get('href'))\n",
        "    print('\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-01T20:50:26.133305Z",
          "start_time": "2022-01-01T20:49:43.865988Z"
        },
        "id": "fkBJX-ASp_Yu"
      },
      "outputs": [],
      "source": [
        "def get_company_news(company_num):\n",
        "    \"\"\"\n",
        "        透過 selenium 網頁操作模擬套件去模擬人工操作，因為 Yahoo Stock 網站有使用非同步更新頁面的方法，\n",
        "        透過 selenium 可以大幅度減低抓取難度，另外，因為新聞內容不需要太頻繁更新(對比即時股價)，\n",
        "        故 selenium 的速度限制將不會是一個問題。\n",
        "    \"\"\"\n",
        "    url = f'https://tw.stock.yahoo.com/quote/{company_num}'\n",
        "    news_urls = []\n",
        "    options = webdriver.ChromeOptions()\n",
        "    options.add_argument(' --headless')   # 背景執行，如果沒有使用，需要把該網頁開始，不然js執行會出現問題\n",
        "    driver = webdriver.Chrome('chromedriver.exe', options=options)  # 預設將 chromedriver.exe 放同一 level \n",
        "    driver.get(url)   # 進入該網頁\n",
        "    time.sleep(5)     # 等待載入完畢, 若沒有等待可能會造成有些元素還沒有respond\n",
        "    \n",
        "    # scroll down\n",
        "    for i in range(10):\n",
        "        print('下拉等待...')\n",
        "        js=\"window.scrollTo(0,document.body.scrollHeight)\"\n",
        "        driver.execute_script(js)\n",
        "        time.sleep(3)\n",
        "        print('下拉完成')\n",
        "    \n",
        "    # 透過BeautifulSoup解析網頁內容\n",
        "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "    \n",
        "    for e in soup.find_all(class_='js-stream-content Pos(r)'):\n",
        "        try:\n",
        "            news_url = e.find_all('a')[0].get('href')\n",
        "            news_source = e.find(class_='C(#959595) Fz(13px) C($c-secondary-text)! D(ib) Mb(6px)').text\n",
        "            news_source = news_source[:news_source.index('•')]\n",
        "            news_title = e.find(class_='Mt(0) Mb(8px)').text\n",
        "            print(news_title, news_source)\n",
        "            news_urls.append((news_url, news_source))\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "    return news_urls\n",
        "\n",
        "\n",
        "company_num = '3105'\n",
        "news_urls = get_company_news(company_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-01T20:51:29.633624Z",
          "start_time": "2022-01-01T20:50:26.137273Z"
        },
        "id": "qi4wIsB9p_Yw"
      },
      "outputs": [],
      "source": [
        "# 單一文章看看可否直接抓出內容，或許搭配 threading 快速下載\n",
        "def is_in_timerange(t, datetime_start, datetime_end):\n",
        "    if datetime_start <= t <= datetime_end:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def get_news(url):\n",
        "    \"\"\"\n",
        "        透過 requests 套件取得網頁內容，再搭配 Beautifulsoup 模組去解析網頁內容，\n",
        "        取得需要的網頁內容: 新聞標題、新聞來源、新聞發布時間、新聞內文。\n",
        "    \"\"\"\n",
        "    # 取得網頁內容\n",
        "    resp = requests.get(url)\n",
        "    soup_news = BeautifulSoup(resp.content, 'html.parser')\n",
        "\n",
        "    # 解析並取得需要的特徵\n",
        "    # 這些資料都需要清理一下\n",
        "    title = soup_news.find(class_='caas-header').text\n",
        "    source = soup_news.find(class_='caas-author-byline-collapse').text\n",
        "    time_text = soup_news.find(class_='caas-attr-time-style').text   # 還需要處理\n",
        "    news_content = soup_news.find(class_='caas-body').text\n",
        "    \n",
        "    print(title, source, time_text)\n",
        "    return title, source, time_text, news_content\n",
        "    \n",
        "    \n",
        "# url = 'https://tw.stock.yahoo.com/news/%E5%85%AC%E5%91%8A-%E7%A9%A9%E6%87%8B%E4%BB%A3%E9%87%8D%E8%A6%81%E5%AD%90%E5%85%AC%E5%8F%B8chainwin-biotech-agrotech-cayman-104429660.html'\n",
        "# get_news(url)\n",
        "rows = []\n",
        "\n",
        "for e in news_urls:\n",
        "    row = list(get_news(e[0]))\n",
        "    row[1] = e[1]\n",
        "    rows.append(row)\n",
        "\n",
        "df = pd.DataFrame(rows, columns=['title', 'source', 'time', 'news_content'])\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-01T20:51:35.530547Z",
          "start_time": "2022-01-01T20:51:35.514510Z"
        },
        "id": "vaVOJvaKp_Yx"
      },
      "outputs": [],
      "source": [
        "# test regex\n",
        "\n",
        "r = \"（中央社記者蔡芃敏台北2021年12月22日電）光電科技工業協進會（PIDA）今天指出，全球...\"\n",
        "reg = re.compile(r'[()（），]')\n",
        "# reg = re.compile('\\\\.+?(?=\\B|$【（：）】，。)')\n",
        "re.sub(reg, string = r, repl = '')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-01T20:51:36.463118Z",
          "start_time": "2022-01-01T20:51:36.390136Z"
        },
        "id": "jGC1rzA9p_Yy"
      },
      "outputs": [],
      "source": [
        "def process_datetime(time_text):\n",
        "    \"\"\"\n",
        "        範例: '2021年6月10日 上午11:51·1 分鐘 (閱讀時間)'\n",
        "    \"\"\"\n",
        "    is_afternoon = False\n",
        "    if '上午' in time_text:\n",
        "        is_afternoon = True\n",
        "        time_text = time_text.replace('上午', '')\n",
        "    else:\n",
        "        time_text = time_text.replace('下午', '')\n",
        "    \n",
        "    if '·' in time_text:\n",
        "        time_text = time_text[:time_text.index('·')]\n",
        "    if is_afternoon:\n",
        "        return datetime.datetime.strptime(time_text, \"%Y年%m月%d日 %H:%M\")\n",
        "    else:\n",
        "        return datetime.datetime.strptime(time_text, \"%Y年%m月%d日 %H:%M\") + datetime.timedelta(hours=8)\n",
        "    \n",
        "    \n",
        "def clean(text):\n",
        "    # 移雛一些特殊符號以及數字，這些透過觀察 raw data 而知\n",
        "    reg = re.compile('[\\\\d\\-)(?=|$【（：）】《，》。]')   \n",
        "    text = text.apply(lambda r: re.sub(reg, string = r, repl = ''))\n",
        "    # 移除 hashtag 的符號'#'\n",
        "    text = text.apply(lambda r: r.replace('#', ''))\n",
        "    # 移除人名標記\n",
        "    reg = re.compile('@\\w+')\n",
        "    text = text.apply(lambda r: re.sub(reg, string = r, repl = '@'))\n",
        "    # 移除網址\n",
        "    reg = re.compile('https?\\S+(?=\\s|$)')\n",
        "    text = text.apply(lambda r: re.sub(reg, string = r, repl = 'www'))\n",
        "    # 全部小寫化\n",
        "    text = text.apply(lambda r: r.upper())\n",
        "    text = text.apply(lambda r: r.replace(' ', ''))\n",
        "    \n",
        "    return text\n",
        "\n",
        "def process_text(df):\n",
        "    \"\"\"\n",
        "        將文字資料根據實際狀況前處理資料。\n",
        "        \n",
        "        步驟:\n",
        "            1. 將 Stop words 移除。\n",
        "            2. 將不應納入考量的資訊移除，如數字、標點符號、特殊符號(已做完)\n",
        "            3. 將一些 domain-know-how 的 word 加入字典(分詞會優先參考): 如行業術語，此處參考 **中文財務情緒字典** \n",
        "            4. 分詞\n",
        "    \"\"\"\n",
        "    # 1.\n",
        "    import jieba.analyse\n",
        "    jieba.analyse.set_stop_words('./datasets/停用詞-繁體中文.txt')\n",
        "    \n",
        "    # 3. 這部分先沒有做，但可以當作擴充。\n",
        "    \n",
        "    # 4.\n",
        "    # jieba.load_userdict('./datasets/new_dict.txt')\n",
        "    df['cut'] = df['cleaned_content'].apply(lambda sentence: ' '.join(list(jieba.cut(sentence))))\n",
        "    \n",
        "    return df\n",
        "    \n",
        "    \n",
        "\n",
        "# process_datetime('2021年6月10日 上午11:51·1 分鐘 (閱讀時間)')\n",
        "df['cleaned_time'] = df['time'].apply(func=lambda x : process_datetime(x))\n",
        "df['datetime_str'] = df['cleaned_time'].apply(func=lambda x : str(x+datetime.timedelta(days=1))[:10])\n",
        "df['cleaned_content'] = clean(df['news_content'])\n",
        "df['cleaned_title'] = clean(df['title'])\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-01T20:51:42.567507Z",
          "start_time": "2022-01-01T20:51:42.507486Z"
        },
        "id": "_YukNYQfp_Yz"
      },
      "outputs": [],
      "source": [
        "# 處理股價資料\n",
        "# 建立 交易信號: (收盤-開盤) / 收盤 * 100\n",
        "\n",
        "def transform_price_to_label(price_trend, threshold):\n",
        "    \"\"\"\n",
        "        將價格趨勢轉換成 label，離散化，這是一個 Reframing 的技巧。\n",
        "    \"\"\"\n",
        "    if price_trend >= threshold:\n",
        "        return 1\n",
        "    elif price_trend < -threshold:\n",
        "        return 0\n",
        "    return -1\n",
        "\n",
        "stocks = pd.read_csv('3105.TWO.csv')\n",
        "stocks['price_trend'] = (stocks['Close'] - stocks['Open']) / stocks['Close'] * 100\n",
        "\n",
        "threshold = 1\n",
        "stocks['label'] = stocks['price_trend'].apply(func=lambda x: transform_price_to_label(x, threshold))\n",
        "stocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-01T20:51:44.905813Z",
          "start_time": "2022-01-01T20:51:44.861785Z"
        },
        "id": "n9TbcvFEp_Y0"
      },
      "outputs": [],
      "source": [
        "# 合併\n",
        "\n",
        "stocks['clean_time'] = stocks['Date'].apply(func=lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d\"))\n",
        "stocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-01T20:51:46.649197Z",
          "start_time": "2022-01-01T20:51:46.636203Z"
        },
        "id": "GTg7uLB0p_Y1"
      },
      "outputs": [],
      "source": [
        "stocks[stocks['Date'] == '2021-01-05'].iloc[0, -2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-01T20:51:49.580764Z",
          "start_time": "2022-01-01T20:51:49.384772Z"
        },
        "id": "d8tKUnA6p_Y1"
      },
      "outputs": [],
      "source": [
        "def get_label(df):\n",
        "    \"\"\"\n",
        "        透過時間判斷搭配stocks資料得取得 label。\n",
        "    \"\"\"\n",
        "    mapping = {}\n",
        "    for datetime_str in stocks['Date']:\n",
        "        #print(datetime_str)\n",
        "        mapping[datetime_str] = stocks[stocks['Date'] == datetime_str].iloc[0, -2]\n",
        "    \n",
        "    df['label'] = df['datetime_str'].apply(func=lambda x: mapping[x] if x in mapping else 0)\n",
        "    \n",
        "get_label(df)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-01T20:51:57.070723Z",
          "start_time": "2022-01-01T20:51:56.891726Z"
        },
        "id": "hYUE5Ax9p_Y2"
      },
      "outputs": [],
      "source": [
        "# 圖示化 標籤分布\n",
        "\n",
        "df_bar = pd.DataFrame({'label': ['漲幅訊號', '平穩訊號', '跌幅訊號'], 'count': [(stocks['label'] == 1).sum(), (stocks['label'] == 0).sum(), (stocks['label'] == -1).sum()]})\n",
        "\n",
        "df_bar.plot.bar(x='label', y='count', rot=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-01T20:52:05.123228Z",
          "start_time": "2022-01-01T20:52:04.206233Z"
        },
        "id": "xnASQPS3p_Y3"
      },
      "outputs": [],
      "source": [
        "# 斷詞\n",
        "\n",
        "df = process_text(df)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-01T20:52:14.532592Z",
          "start_time": "2022-01-01T20:52:14.470590Z"
        },
        "id": "T2sRuxQup_Y4"
      },
      "outputs": [],
      "source": [
        "# 向量化: tf-idf\n",
        "\n",
        "# corpus = df.ilo\n",
        "corpus = df.loc[:, 'cut'].values\n",
        "vectorizer = TfidfVectorizer(max_features=3000)\n",
        "x_tf_idf = vectorizer.fit_transform(corpus)\n",
        "vectorizer.get_feature_names()\n",
        "print(x_tf_idf.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-01T20:52:20.152498Z",
          "start_time": "2022-01-01T20:52:20.096500Z"
        },
        "id": "0-5dRavQp_Y5"
      },
      "outputs": [],
      "source": [
        "# 訓練資料\n",
        "\n",
        "X = pd.DataFrame(x_tf_idf.toarray(), columns=vectorizer.get_feature_names())\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-01T20:53:00.178261Z",
          "start_time": "2022-01-01T20:52:44.799169Z"
        },
        "id": "0zUUpH42p_Y6"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, f1_score, auc\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "models = {\n",
        "    'rf': RandomForestClassifier(),\n",
        "    'gdbt': GradientBoostingClassifier(),\n",
        "    'lr': LogisticRegression(),\n",
        "    'svm': SVC(),\n",
        "    'nb': MultinomialNB()\n",
        "}\n",
        "models_result = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(name)\n",
        "    scores = cross_val_score(estimator = model, X=X, y=df['label'], cv=5)\n",
        "    print(scores)\n",
        "    print(f'平均acc: {np.mean(scores)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQpT_JeCp_Y7"
      },
      "source": [
        "> 基本上，因為本身資料少又有缺陷，對於建立一個基於此的模型，效果自然不會太好。 之後用預訓練模型可能會有更好的效果。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-01T20:53:13.468840Z",
          "start_time": "2022-01-01T20:53:09.045097Z"
        },
        "id": "Tha7f0Vep_Y7"
      },
      "outputs": [],
      "source": [
        "# wordcloud based on tf-idf\n",
        "\n",
        "# corpus = ['Hi what are you accepting here do you accept me',\n",
        "# 'What are you thinking about getting today',\n",
        "# 'Give me your password to get accepted into this school',\n",
        "# 'The man went to the tree to get his sword back',\n",
        "# 'go away to a far away place in a foreign land']\n",
        "\n",
        "# vectorizer = TfidfVectorizer(stop_words='english')\n",
        "# vecs = vectorizer.fit_transform(corpus)\n",
        "# feature_names = vectorizer.get_feature_names()\n",
        "\n",
        "\n",
        "dense = x_tf_idf.todense()\n",
        "lst1 = dense.tolist()\n",
        "df_cloud = pd.DataFrame(lst1, columns=vectorizer.get_feature_names())\n",
        "# df.T.sum(axis=1)\n",
        "\n",
        "cloud = WordCloud(background_color=\"white\", max_words=200, font_path = \"C:\\Windows\\Fonts\\kaiu\").generate_from_frequencies(df_cloud.T.sum(axis=1))\n",
        "plt.imshow(cloud.to_file('文字雲.png'))\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-01T21:02:29.376671Z",
          "start_time": "2022-01-01T21:02:17.541255Z"
        },
        "id": "ez6pufWnp_Y8"
      },
      "outputs": [],
      "source": [
        "# 詞頻版本\n",
        "\n",
        "import wordcloud\n",
        "\n",
        "def draw_wordcloud(corpus):\n",
        "    \"\"\"\n",
        "        functionality:\n",
        "            draw the wordcloud\n",
        "        Args:\n",
        "            corpus: str\n",
        "    \"\"\"\n",
        "    words = list(jieba.cut(corpus, cut_all = False))   # 分詞\n",
        "    #words = [e for e in words if e.isalpha()]        # \n",
        "    words = [e for e in words if len(e) != 1]        # 只要長度是2以上的word\n",
        "    words = \" \".join(words)\n",
        "    \n",
        "    cloud = wordcloud.WordCloud(background_color = \"black\",\n",
        "                                font_path = \"C:\\Windows\\Fonts\\kaiu\",   # 字型路徑\n",
        "                                scale = 2,                             # 調整解析度\n",
        "                                width = 1000,                          # 圖片長度\n",
        "                                height = 600,                          # 圖片寬度\n",
        "                                min_font_size = 20,\n",
        "                                max_words = 200)\n",
        "    cloud = cloud.generate(words)                    # 產生文字雲\n",
        "    plt.figure(figsize = (15, 15))\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(cloud)\n",
        "    plt.show()\n",
        "\n",
        "    \n",
        "def get_class_wordcloud(df):\n",
        "    for i in df['label'].unique():\n",
        "        print(i)\n",
        "        corpus = []\n",
        "        temp_df = df[(df['label'] == i)]\n",
        "        for j in range(len(temp_df)):\n",
        "            corpus.append(temp_df.iloc[j, -1])\n",
        "        corpus = ''.join(corpus)\n",
        "        draw_wordcloud(corpus)\n",
        "        \n",
        "        \n",
        "get_class_wordcloud(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-01T21:13:11.627908Z",
          "start_time": "2022-01-01T21:13:09.047367Z"
        },
        "id": "99_h0BAVp_Y-"
      },
      "outputs": [],
      "source": [
        "import lda\n",
        "from sklearn.feature_extraction.text import CountVectorizer \n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X_bow = vectorizer.fit_transform(corpus)\n",
        "word = vectorizer.get_feature_names()   # 所有的特征词，即关键词\n",
        "print (word)    \n",
        "\n",
        "\n",
        "# 訓練模型\n",
        "weight = X_bow.toarray()\n",
        "model = lda.LDA(n_topics = 3, n_iter = 500, random_state = 1)\n",
        "model.fit(np.asarray(weight))\n",
        "# 主题-詞 分布\n",
        "topic_word = model.topic_word_  # 生成主题以及主题中词的分布\n",
        "print(\"topic-word:\\n\", topic_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-01T21:14:00.602911Z",
          "start_time": "2022-01-01T21:13:13.484983Z"
        },
        "id": "rdmY51Nyp_Y_"
      },
      "outputs": [],
      "source": [
        "# 计算topN关键词\n",
        "n = 5    \n",
        "for i, word_weight in enumerate(topic_word):  \n",
        "    print(\"word_weight:\\n\", word_weight)\n",
        "    distIndexArr = np.argsort(word_weight)\n",
        "    print(\"distIndexArr:\\n\", distIndexArr)\n",
        "    topN_index = distIndexArr[:-(n+1):-1]\n",
        "    print(\"topN_index:\\n\", topN_index) # 权重最在的n个\n",
        "    topN_words = np.array(word)[topN_index]    \n",
        "    print(u'*Topic {}\\n- {}'.format(i, ' '.join(topN_words))) \n",
        "# 绘制主题-词分布图\n",
        "import matplotlib.pyplot as plt  \n",
        "f, ax= plt.subplots(2, 1, figsize=(6, 6), sharex=True)  \n",
        "for i, k in enumerate([0, 1]):         #两个主题\n",
        "    ax[i].stem(topic_word[k,:], linefmt='b-',  \n",
        "               markerfmt='bo', basefmt='w-')  \n",
        "    ax[i].set_xlim(-2,20)  \n",
        "    ax[i].set_ylim(0, 1)  \n",
        "    ax[i].set_ylabel(\"Prob\")  \n",
        "    ax[i].set_title(\"topic {}\".format(k))  \n",
        "ax[1].set_xlabel(\"word\")  \n",
        "plt.tight_layout()  \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-01T21:14:00.618918Z",
          "start_time": "2022-01-01T21:14:00.605914Z"
        },
        "id": "IiokOV3hp_Y_"
      },
      "outputs": [],
      "source": [
        "# 文档-主题分布  \n",
        "doc_topic = model.doc_topic_ \n",
        "print(\"type(doc_topic): {}\".format(type(doc_topic)))  \n",
        "print(\"shape: {}\".format(doc_topic.shape)) \n",
        "label = []        \n",
        "for i in range(10):  \n",
        "    print(doc_topic[i])\n",
        "    topic_most_pr = doc_topic[i].argmax()  \n",
        "    label.append(topic_most_pr)  \n",
        "    print(\"doc: {} topic: {}\".format(i, topic_most_pr))  \n",
        "print(label)    # 前10篇文章对应的主题列表"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-01T21:14:01.682239Z",
          "start_time": "2022-01-01T21:14:00.621918Z"
        },
        "id": "lkXH2rH-p_ZA"
      },
      "outputs": [],
      "source": [
        "# 绘制文档-主题分布图  \n",
        "import matplotlib.pyplot as plt    \n",
        "f, ax= plt.subplots(6, 1, figsize=(8, 8), sharex=True)    \n",
        "for i, k in enumerate([0,1,2,3,8,9]):    \n",
        "    ax[i].stem(doc_topic[k,:], linefmt='r-',    \n",
        "               markerfmt='ro', basefmt='w-')    \n",
        "    ax[i].set_xlim(-1, 2)     #x坐标下标  \n",
        "    ax[i].set_ylim(0, 1.2)    #y坐标下标  \n",
        "    ax[i].set_ylabel(\"Probability\")    \n",
        "    ax[i].set_title(\"Document {}\".format(k))    \n",
        "ax[5].set_xlabel(\"Topic\")  \n",
        "plt.tight_layout()  \n",
        "plt.show() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgdErm9xp_ZB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}